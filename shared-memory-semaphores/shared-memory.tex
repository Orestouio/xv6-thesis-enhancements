\documentclass[12pt]{article}

% Including necessary packages for document structure and formatting
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage{parskip}

% Configure listings package for code snippets
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  columns=fullflexible,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
  tabsize=2,
  showspaces=false,
  showstringspaces=false,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{purple},
  language=C
}

% Configure hyperlinks
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}

% Set up the document header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Shared Memory and Semaphore Implementation in xv6}
\fancyfoot[C]{\thepage}

% Start the document
\begin{document}

% Title Page
\title{Shared Memory and Semaphore Implementation in xv6}
\author{Orestis Theodorou}
\date{May 6, 2025}
\maketitle

\tableofcontents
\newpage

\section{Background and Related Work}
\label{sec:background}

The xv6 operating system, developed by MIT \cite{xv6}, is a pedagogical platform derived from UNIX v6, designed for teaching operating system concepts. Its original design lacks advanced inter-process communication (IPC) mechanisms and synchronization primitives, limiting its ability to support concurrent programming scenarios.

Shared memory is a fundamental IPC mechanism, as described by Silberschatz et al. \cite{silberschatz2018operating}, allowing processes to share data efficiently by mapping a common memory region into their address spaces. Semaphores, introduced by Dijkstra \cite{dijkstra1965cooperating}, provide a synchronization primitive to coordinate access to shared resources, preventing race conditions in concurrent environments. These mechanisms are essential for scenarios like the producer-consumer problem, a classic synchronization challenge in operating systems.

This work implements shared memory and semaphores in xv6, enabling efficient IPC and synchronization, and validates their functionality through a producer-consumer application, addressing a gap in xv6’s original design.

\section{Shared Memory and Semaphore Implementation in xv6}
\label{sec:implementation}

This section delineates the design, implementation, and rigorous testing of shared memory and semaphore mechanisms within xv6, a pedagogical operating system. Shared memory serves as an efficient inter-process communication (IPC) primitive, enabling multiple processes to access a common memory region for seamless data sharing. Semaphores, as a synchronization mechanism, ensure coordinated access to these shared resources. Together, these components facilitate a robust producer-consumer application, exemplifying practical IPC and synchronization in xv6.

\subsection{Shared Memory Implementation}
\label{subsec:shared-memory-implementation}

\subsubsection{Design Overview}
\label{subsubsec:shared-memory-design}

The shared memory system in xv6 is engineered to enable processes to create, access, and manage shared memory regions identified by unique string identifiers. The system is designed with the following key features:
\begin{itemize}
  \item \textbf{Named Shared Memory Regions}: Processes can create or access shared memory regions using a string identifier (e.g., \texttt{/shm1}).
  \item \textbf{Multiple Mappings}: Each process can map up to a predefined number of shared memory regions into its address space.
  \item \textbf{Inheritance Across Fork}: Shared memory mappings are inherited by child processes during a \texttt{fork} system call, ensuring seamless communication between parent and child processes.
  \item \textbf{Reference Counting}: Shared memory regions are automatically deallocated when no processes reference them, ensuring efficient resource management.
\end{itemize}

The shared memory implementation is supported by two primary data structures:
\begin{itemize}
  \item \textbf{Shared Memory Table (\texttt{shmtable})}: A global array of \texttt{NSHM} (set to 10) \texttt{shm} structures, each representing a shared memory object. The \texttt{shm} structure, defined in \texttt{proc.h}, comprises:
    \begin{itemize}
      \item \texttt{name}: A 16-character array storing the identifier of the shared memory region.
      \item \texttt{in\_use}: A flag indicating whether the slot is occupied.
      \item \texttt{phys\_addr}: The physical address of the allocated memory.
      \item \texttt{size}: The size of the memory region (in bytes).
      \item \texttt{ref\_count}: The number of processes mapping this region.
      \item \texttt{lock}: A spinlock ensuring thread-safe access.
    \end{itemize}
  \item \textbf{Per-Process Mappings}: The \texttt{proc} structure, defined in \texttt{proc.h}, is extended to include:
    \begin{itemize}
      \item \texttt{shm\_mappings[MAX\_SHM\_MAPPINGS]}: An array of virtual addresses where shared memory regions are mapped (up to \texttt{MAX\_SHM\_MAPPINGS}, set to 4).
      \item \texttt{shm\_objects[MAX\_SHM\_MAPPINGS]}: An array of pointers to the corresponding \texttt{shm} structures.
      \item \texttt{shm\_count}: The number of active shared memory mappings in the process.
    \end{itemize}
\end{itemize}

The shared memory system provides two system calls to user space:
\begin{itemize}
  \item \texttt{shm\_open(const char *name, int size)}: Creates or opens a shared memory region identified by \texttt{name} with the specified \texttt{size}, mapping it into the process’s address space and returning the virtual address.
  \item \texttt{shm\_close(int addr)}: Unmaps the shared memory region at the specified virtual \texttt{addr} from the process’s address space, deallocating it if no other processes reference it.
\end{itemize}

\subsubsection{Implementation Details}
\label{subsubsec:shared-memory-implementation-details}

The shared memory system is implemented within the xv6 kernel through modifications to several files:

\begin{itemize}
  \item \textbf{proc.h}: Defines the \texttt{shm} structure and extends the \texttt{proc} structure with fields for managing shared memory mappings.
  \item \textbf{proc.c}: Implements the system call handlers \texttt{sys\_shm\_open} and \texttt{sys\_shm\_close}, along with modifications to \texttt{fork} and \texttt{exit} to handle shared memory mappings.
  \item \textbf{syscall.h} and \textbf{syscall.c}: Define system call numbers (\texttt{SYS\_shm\_open}, \texttt{SYS\_shm\_close}) and map them to their handlers.
  \item \textbf{usys.S} and \textbf{user.h}: Provide user-space wrappers and prototypes for \texttt{shm\_open} and \texttt{shm\_close}.
\end{itemize}

The \texttt{sys\_shm\_open} system call executes the following steps:
\begin{enumerate}
  \item \textbf{Argument Retrieval}: Extracts the \texttt{name} and \texttt{size} arguments using \texttt{argstr} and \texttt{argint}.
  \item \textbf{Input Validation}: Ensures \texttt{size} is positive and does not exceed \texttt{PGSIZE} (4096 bytes).
  \item \textbf{Shared Memory Lookup}: Searches \texttt{shmtable} for an existing shared memory object with the given \texttt{name}. If found, reuses it; otherwise, allocates a new slot.
  \item \textbf{Memory Allocation}: Allocates a physical page for new shared memory objects using \texttt{kalloc} and initializes it to zero.
  \item \textbf{Virtual Address Assignment}: Maps the shared memory at a virtual address starting at \texttt{0x60000000}, incremented by \texttt{PGSIZE} for each mapping in the process (e.g., \texttt{0x60001000} for the second mapping).
  \item \textbf{Page Table Mapping}: Uses \texttt{mappages} to map the virtual address to the physical address with user and write permissions.
  \item \textbf{Reference Management}: Increments the \texttt{ref\_count} of the shared memory object and updates the process’s \texttt{shm\_mappings} and \texttt{shm\_objects}.
\end{enumerate}

The \texttt{sys\_shm\_close} system call:
\begin{enumerate}
  \item \textbf{Argument Retrieval}: Extracts the virtual \texttt{addr} using \texttt{argint}.
  \item \textbf{Mapping Lookup}: Searches the process’s \texttt{shm\_mappings} for the given \texttt{addr}.
  \item \textbf{Page Table Unmapping}: Clears the page table entry for the virtual address.
  \item \textbf{Reference Management}: Decrements the \texttt{ref\_count} of the shared memory object and deallocates the physical memory if the count reaches zero.
  \item \textbf{Cleanup}: Updates the process’s \texttt{shm\_mappings} and \texttt{shm\_count}.
\end{enumerate}

To support shared memory across process boundaries:
\begin{itemize}
  \item \textbf{fork}: Copies the parent’s \texttt{shm\_mappings}, \texttt{shm\_objects}, and \texttt{shm\_count} to the child, explicitly mapping each shared memory region into the child’s page table using \texttt{mappages} and incrementing the \texttt{ref\_count}.
  \item \textbf{exit}: Unmaps all shared memory regions from the process’s address space, ensuring proper cleanup of mappings and deallocation of shared memory objects.
\end{itemize}

\subsubsection{Testing and Validation}
\label{subsubsec:shared-memory-testing}

An initial test program, \texttt{shmtest.c}, was developed to validate the basic functionality of the shared memory system. The program opens a shared memory region named \texttt{/shm1}, forks a child process, and performs the following:
\begin{itemize}
  \item The parent initializes the shared memory to 0.
  \item The child reads the initial value, writes 42, and closes the region.
  \item The parent waits for the child, reads the updated value, and closes the region.
\end{itemize}
The test confirmed that both processes could access the shared memory, with the parent observing the child’s update (output: \texttt{Child: Shared memory value = 0}, \texttt{Child: Set shared memory to 42}, \texttt{Parent: Shared memory value = 42}, \texttt{[Kernel Debug] Shared memory /shm1 freed}).

The test program was extended to validate additional scenarios, with the updated version overwriting the original \texttt{shmtest.c}. The extended test includes the following cases:
\begin{itemize}
  \item \textbf{Multiple Shared Memory Regions}: Opens two regions (\texttt{/shm1}, \texttt{/shm2}) in both parent and child processes, verifying isolation and correct updates.
  \item \textbf{Maximum Mappings}: Attempts to map \texttt{MAX\_SHM\_MAPPINGS + 1} (5) regions, confirming that the limit (\texttt{MAX\_SHM\_MAPPINGS = 4}) is enforced.
  \item \textbf{Reusing Names}: Verifies that multiple processes can share the same named region (\texttt{/shm\_reuse}), with updates visible to all and cleanup occurring only after the last process closes the region.
  \item \textbf{Invalid Inputs}: Ensures that \texttt{shm\_open} fails for invalid sizes (negative, zero, or exceeding \texttt{PGSIZE}) and \texttt{shm\_close} fails for invalid addresses.
\end{itemize}

The output of the extended \texttt{shmtest} program is as follows:
\begin{lstlisting}
Test 1: Opening two shared memory regions
Parent: Set /shm1 to 100, /shm2 to 200
Child: /shm1 = 100, /shm2 = 200
Child: Set /shm1 to 101, /shm2 to 201
Parent: /shm1 = 101, /shm2 = 201
[Kernel Debug] Shared memory /shm1 freed
[Kernel Debug] Shared memory /shm2 freed

Test 2: Maximum shared memory mappings
Opened /shm_max0 at address 0x60000000
Opened /shm_max1 at address 0x60001000
Opened /shm_max2 at address 0x60002000
Opened /shm_max3 at address 0x60003000
shm_open failed for /shm_max4 (expected for i=4)
[Kernel Debug] Shared memory /shm_max0 freed
[Kernel Debug] Shared memory /shm_max1 freed
[Kernel Debug] Shared memory /shm_max2 freed
[Kernel Debug] Shared memory /shm_max3 freed

Test 3: Reusing shared memory names
Child: /shm_reuse = 300
Child: Set /shm_reuse to 301
Parent: /shm_reuse = 301
[Kernel Debug] Shared memory /shm_reuse freed

Test 4: Invalid inputs
shm_open with negative size failed (expected)
shm_open with zero size failed (expected)
shm_open with size > PGSIZE failed (expected)
shm_close with invalid address failed (expected)

All tests completed
\end{lstlisting}

\subsubsection{Challenges and Solutions}
\label{subsubsec:shared-memory-challenges}

Several challenges were encountered during the implementation of the shared memory system:
\begin{itemize}
  \item \textbf{Implicit Declaration of \texttt{mappages}}: Resolved by removing the \texttt{static} keyword from \texttt{mappages} in \texttt{vm.c} and ensuring its prototype in \texttt{defs.h}.
  \item \textbf{Undefined \texttt{strcmp}}: Replaced with \texttt{strncmp} due to the absence of \texttt{strcmp} in \texttt{string.c}, using a length limit for safety.
  \item \textbf{User-Space System Call Errors}: Defined \texttt{SYS\_shm\_open} and \texttt{SYS\_shm\_close} in \texttt{syscall.h}, added wrappers in \texttt{usys.S}, and removed inline assembly from \texttt{ulib.c}.
  \item \textbf{Multiple Definitions}: Removed redundant \texttt{shm\_open} and \texttt{shm\_close} definitions in \texttt{ulib.c} to avoid conflicts with \texttt{usys.S}.
  \item \textbf{Page Fault in Child Process}: Modified \texttt{fork} to explicitly copy shared memory mappings, ensuring the child inherits the mappings.
  \item \textbf{Missing \texttt{snprintf} in User Space}: The extended test initially used \texttt{snprintf} to generate shared memory names dynamically. Since \texttt{snprintf} is not available in xv6, it was replaced with manual string construction using \texttt{strcpy} and direct character manipulation to append digits.
\end{itemize}

\subsection{Semaphore Implementation}
\label{subsec:semaphore-implementation}

\subsubsection{Design Overview}
\label{subsubsec:semaphore-design}

The semaphore system in xv6 is designed to provide a robust synchronization primitive for coordinating access to shared resources, such as the shared memory regions implemented previously. The system supports the following key features:
\begin{itemize}
  \item \textbf{Counting Semaphores}: Semaphores maintain a non-negative integer value, enabling both mutual exclusion (binary semaphore) and resource counting (general semaphore).
  \item \textbf{Blocking and Waking}: Processes waiting on a semaphore with a value of 0 are blocked until another process signals the semaphore, ensuring proper synchronization.
  \item \textbf{Inheritance Across Fork}: Semaphore associations are inherited by child processes during a \texttt{fork} operation, maintaining synchronization across process boundaries.
  \item \textbf{Automatic Cleanup}: Semaphores are automatically deallocated when a process exits, with proper handling of waiting processes to prevent deadlocks.
  \item \textbf{Error Handling}: The system includes mechanisms to handle invalid semaphore IDs and queue overflows, ensuring robust operation under erroneous conditions.
\end{itemize}

The semaphore implementation relies on the following data structures:
\begin{itemize}
  \item \textbf{Semaphore Table (\texttt{semtable})}: A global array of \texttt{NSEM} (set to 10) \texttt{sem} structures, each representing a semaphore. The \texttt{sem} structure, defined in \texttt{proc.h}, includes:
    \begin{itemize}
      \item \texttt{in\_use}: A flag indicating whether the semaphore slot is occupied.
      \item \texttt{value}: The current value of the semaphore, used for counting available resources.
      \item \texttt{lock}: A spinlock ensuring thread-safe access to the semaphore’s state.
      \item \texttt{queue}: An array of \texttt{NPROC} (64) pointers to \texttt{proc} structures, implementing a circular wait queue with \texttt{queue\_head} and \texttt{queue\_tail} indices to manage blocked processes.
    \end{itemize}
  \item \textbf{Per-Process Semaphore Tracking}: The \texttt{proc} structure is extended to include:
    \begin{itemize}
      \item \texttt{sem\_ids[MAX\_SEM]}: An array of semaphore IDs (indices into \texttt{semtable}) associated with the process (up to \texttt{MAX\_SEM}, set to 4).
      \item \texttt{sem\_count}: The number of semaphores currently associated with the process.
    \end{itemize}
\end{itemize}

The semaphore system provides three system calls to user space:
\begin{itemize}
  \item \texttt{sem\_init(int value)}: Allocates a semaphore, initializes it with the given \texttt{value}, and returns its ID (index in \texttt{semtable}).
  \item \texttt{sem\_wait(int sem\_id)}: Decrements the semaphore’s value; if the value becomes negative, the process blocks until the semaphore is signaled.
  \item \texttt{sem\_post(int sem\_id)}: Increments the semaphore’s value; if there are waiting processes, wakes one up.
\end{itemize}

\subsubsection{Implementation Details}
\label{subsubsec:semaphore-implementation-details}

The semaphore system is implemented within the xv6 kernel through modifications to several files:

\begin{itemize}
  \item \textbf{proc.h}: Defines the \texttt{sem} structure and extends the \texttt{proc} structure with fields for managing semaphores.
  \item \textbf{proc.c}: Implements the system call handlers \texttt{sys\_sem\_init}, \texttt{sys\_sem\_wait}, and \texttt{sys\_sem\_post}, along with modifications to \texttt{fork} and \texttt{exit} to handle semaphores.
  \item \textbf{syscall.h} and \textbf{syscall.c}: Define system call numbers (\texttt{SYS\_sem\_init}, \texttt{SYS\_sem\_wait}, \texttt{SYS\_sem\_post}) and map them to their handlers.
  \item \textbf{usys.S} and \textbf{user.h}: Provide user-space wrappers and prototypes for \texttt{sem\_init}, \texttt{sem\_wait}, and \texttt{sem\_post}.
\end{itemize}

The \texttt{sys\_sem\_init} system call:
\begin{enumerate}
  \item \textbf{Argument Retrieval}: Extracts the initial \texttt{value} using \texttt{argint}.
  \item \textbf{Input Validation}: Ensures \texttt{value} is non-negative and the process has not exceeded \texttt{MAX\_SEM} (4) semaphores; returns -1 if either condition is violated.
  \item \textbf{Semaphore Allocation}: Searches \texttt{semtable} for a free slot, marks it as \texttt{in\_use}, and returns -1 if no slots are available.
  \item \textbf{Initialization}: Sets the semaphore’s \texttt{value}, initializes the wait queue (\texttt{queue\_head} and \texttt{queue\_tail} set to 0), and adds the semaphore ID to the process’s \texttt{sem\_ids}.
\end{enumerate}

The \texttt{sys\_sem\_wait} system call:
\begin{enumerate}
  \item \textbf{Argument Retrieval}: Extracts the \texttt{sem\_id} using \texttt{argint}.
  \item \textbf{Validation}: Ensures the \texttt{sem\_id} is within the valid range (0 to \texttt{NSEM-1}) and the semaphore is in use; returns -1 if invalid.
  \item \textbf{Semaphore Operation}: Decrements the semaphore’s \texttt{value} under the protection of \texttt{semtable[sem\_id].lock}.
  \item \textbf{Blocking}: If \texttt{value} becomes negative, checks if the wait queue is full (returns -1 if so), adds the process to the queue by updating \texttt{queue\_tail}, and calls \texttt{sleep} to block until signaled.
\end{enumerate}

The \texttt{sys\_sem\_post} system call:
\begin{enumerate}
  \item \textbf{Argument Retrieval}: Extracts the \texttt{sem\_id} using \texttt{argint}.
  \item \textbf{Validation}: Ensures the \texttt{sem\_id} is within the valid range and the semaphore is in use; returns -1 if invalid.
  \item \textbf{Semaphore Operation}: Increments the semaphore’s \texttt{value} under the protection of \texttt{semtable[sem\_id].lock}.
  \item \textbf{Waking}: If there are waiting processes (indicated by \texttt{value} $\leq$ 0), removes the process at \texttt{queue\_head} from the queue, updates \texttt{queue\_head}, and calls \texttt{wakeup} after releasing the semaphore lock to make the process runnable.
\end{enumerate}

To support semaphores across process boundaries:
\begin{itemize}
  \item \textbf{fork}: Copies the parent’s \texttt{sem\_ids} and \texttt{sem\_count} to the child, ensuring the child inherits the same semaphore associations without modifying the semaphore state in \texttt{semtable}.
  \item \textbf{exit}: Deallocates all semaphores associated with the process by clearing the wait queue (waking any waiting processes using \texttt{wakeup}), marking the semaphore slot as not \texttt{in\_use}, and resetting its state.
\end{itemize}

\subsubsection{Testing and Validation}
\label{subsubsec:semaphore-testing}

A producer-consumer test program, \texttt{prodcons.c}, was developed to validate the semaphore implementation comprehensively. The program uses shared memory to create a circular buffer and employs three semaphores: \texttt{empty} (initially equal to the buffer size) to track empty slots, \texttt{full} (initially 0) to track filled slots, and \texttt{print\_sem} (initially 1) to synchronize console output. The test program was executed under multiple scenarios to ensure robustness:

\textbf{Standard Test (Buffer Size 5, 10 Items):} The producer generates 10 items, and the consumer reads 10 items, using a circular buffer of size 5. The output of this test is as follows:
\begin{lstlisting}
Producer: produced 0 at index 0
Consumer: consumed 0 from index 0
Producer: produced 1 at index 1
Consumer: consumed 1 from index 1
Producer: produced 2 at index 2
Consumer: consumed 2 from index 2
Producer: produced 3 at index 3
Consumer: consumed 3 from index 3
Producer: produced 4 at index 4
Consumer: consumed 4 from index 4
Producer: produced 5 at index 0
Consumer: consumed 5 from index 0
Producer: produced 6 at index 1
Consumer: consumed 6 from index 1
Producer: produced 7 at index 2
Consumer: consumed 7 from index 2
Producer: produced 8 at index 3
Consumer: consumed 8 from index 3
Producer: produced 9 at index 4
Consumer: consumed 9 from index 4
[Kernel Debug] Semaphore 0 freed
[Kernel Debug] Semaphore 1 freed
[Kernel Debug] Semaphore 2 freed
[Kernel Debug] Shared memory /buffer freed
\end{lstlisting}

\textbf{Stress Test (Buffer Size 2, 20 Items):} To evaluate the semaphore implementation under high contention, the buffer size was reduced to 2, and the number of items was increased to 20. This forces the producer to block frequently when the buffer is full, testing the semaphore’s blocking and waking mechanisms. A representative portion of the output is:
\begin{lstlisting}
Producer: produced 0 at index 0
Consumer: consumed 0 from index 0
Producer: produced 1 at index 1
Consumer: consumed 1 from index 1
Producer: produced 2 at index 0
Consumer: consumed 2 from index 0
Producer: produced 3 at index 1
Consumer: consumed 3 from index 1
Producer: produced 4 at index 0
Consumer: consumed 4 from index 0
[...]
Producer: produced 19 at index 1
Consumer: consumed 19 from index 1
[Kernel Debug] Semaphore 0 freed
[Kernel Debug] Semaphore 1 freed
[Kernel Debug] Semaphore 2 freed
[Kernel Debug] Shared memory /buffer freed
\end{lstlisting}

\textbf{Error Handling Test (Invalid Semaphore ID):} An additional test case was added to \texttt{prodcons.c} to verify error handling by attempting to use an invalid semaphore ID (-1) for \texttt{sem\_wait} and \texttt{sem\_post}. The program correctly detects the invalid ID and exits with an error message:
\begin{lstlisting}
prodcons: invalid semaphore ID -1, exiting
\end{lstlisting}

The standard test demonstrates that the producer and consumer access the buffer in a synchronized manner, with items produced and consumed in the correct order. The semaphores ensure that the producer blocks when the buffer is full and the consumer blocks when the buffer is empty, preventing race conditions. The \texttt{print\_sem} semaphore eliminates interleaved console output, ensuring clear and readable logs. A small delay in the producer loop balances execution, making the synchronization behavior evident.

The stress test confirms the semaphore’s robustness under high contention, as the producer and consumer continue to operate correctly despite frequent blocking and waking, with no deadlocks or race conditions observed. The error handling test validates that the system gracefully handles invalid inputs, returning appropriate error codes and preventing undefined behavior.

\subsubsection{Challenges and Solutions}
\label{subsubsec:semaphore-challenges}

Several challenges were encountered during the semaphore implementation:

\begin{itemize}
  \item \textbf{Linker Errors for Semaphore System Calls}: Initial linker errors (\texttt{undefined reference to `sys\_sem\_init'}, etc.) arose because the implementations of \texttt{sys\_sem\_init}, \texttt{sys\_sem\_wait}, and \texttt{sys\_sem\_post} were missing in \texttt{proc.c}, despite being declared in \texttt{syscall.c}. This was resolved by implementing these system calls in \texttt{proc.c}.
  \item \textbf{Kernel Panic Due to Lock Acquisition}: A \texttt{panic: acquire} error occurred because \texttt{sys\_sem\_wait} and \texttt{sys\_sem\_post} held both \texttt{ptable.lock} and \texttt{semtable[sem\_id].lock} while calling \texttt{sleep} and \texttt{wakeup}, violating xv6’s locking rules. The issue was mitigated by restructuring the system calls to release \texttt{ptable.lock} before acquiring \texttt{semtable[sem\_id].lock} and ensuring \texttt{wakeup} is called after releasing all locks.
  \item \textbf{Interleaved Console Output}: The initial \texttt{prodcons} output was jumbled due to concurrent \texttt{printf} calls from the producer and consumer. A third semaphore, \texttt{print\_sem}, was introduced to synchronize console output, ensuring atomic message printing.
  \item \textbf{Unbalanced Execution}: The producer initially outpaced the consumer, filling the buffer before the consumer could consume items. A \texttt{sleep(1)} delay was added to the producer loop to balance execution, resulting in clearer interleaving of actions in the output.
\end{itemize}

\subsection{Conclusion}
\label{subsec:conclusion}

The shared memory and semaphore implementations in xv6 establish a robust and reliable framework for inter-process communication and synchronization. The shared memory system facilitates efficient data sharing through named regions, supports inheritance across \texttt{fork}, and ensures proper resource management, as validated by the \texttt{shmtest} program across diverse scenarios, including edge cases such as maximum mappings and invalid inputs. The semaphore system complements this by providing a synchronization mechanism that ensures coordinated access to shared resources, as demonstrated by the \texttt{prodcons} program. The producer-consumer application successfully operates under both standard and stress conditions, with a buffer size of 5 for normal operation and a reduced size of 2 for high-contention scenarios, producing and consuming 20 items without deadlocks or race conditions. Error handling tests further confirm the system’s resilience by appropriately managing invalid semaphore IDs.

The successful implementation of the producer-consumer application underscores the practical utility of these mechanisms in xv6. It exemplifies a classic synchronization problem, demonstrating how semaphores can manage shared resources in a multi-process environment, ensuring data consistency and preventing race conditions. The consistent behavior across multiple test runs, including stress tests, highlights the scalability and reliability of the implementation, making it suitable for educational purposes and as a foundation for more complex operating system features.

Future enhancements could further expand the capabilities of this system. Stress testing with multiple producer-consumer pairs sharing the same buffer could provide deeper insights into the semaphore’s performance under concurrent access by numerous processes. Implementing a \texttt{sem\_destroy} system call would allow explicit semaphore cleanup, offering finer control over resource management. Additionally, integrating advanced synchronization primitives, such as condition variables or reader-writer locks, could enable more sophisticated synchronization patterns, such as those required in multi-threaded applications or database systems. Exploring dynamic resizing of the \texttt{semtable} and \texttt{shmtable} arrays could address the current static limits (\texttt{NSHM} and \texttt{NSEM} set to 10), enhancing scalability. These improvements would not only strengthen xv6’s IPC and synchronization capabilities but also provide valuable learning opportunities for understanding operating system design and implementation.

\section{Repository}
The complete implementation—including \texttt{proc.c}, \texttt{proc.h}, \texttt{shmtest.c}, \texttt{prodcons.c}, and supporting files—is available at \\
\url{https://github.com/Orestouio/Xv6_SharedMemorySemaphores}.

\begin{thebibliography}{9}
\bibitem{xv6}
Cox, R., Kaashoek, F., \& Morris, R. (2019). Xv6: A Simple, Unix-like Teaching Operating System. \textit{MIT CSAIL}.
\bibitem{silberschatz2018operating}
Silberschatz, A., Galvin, P. B., \& Gagne, G. (2018). \textit{Operating System Concepts}. Wiley.
\bibitem{dijkstra1965cooperating}
Dijkstra, E. W. (1965). Cooperating Sequential Processes. \textit{Programming Languages}, Academic Press.
\end{thebibliography}

\end{document}